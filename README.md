# Paper-Implementations
My implementation of Machine Learning and Deep Learning papers from scratch.

| Paper Name | Link to Paper | Year Published | GitHub Folder |
|------------|----------------|---------------|---------------|
| Improving Language Understanding by Generative Pre-Training| [GPT Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | 2018 | [GPT Implementation](./GPT) |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [BERT Paper](https://arxiv.org/abs/1810.04805) | 2019 | [BERT Implementation](./BERT) |
| Language Models are Unsupervised Multitask Learners | [GPT2 Paper](https://arxiv.org/abs/1810.04805) | 2019 | [GPT2 Implementation](./GPT2) |
| LoRA: Low-Rank Adaptation of Large Language Models | [LoRA Paper](https://arxiv.org/abs/2106.09685) | 2021 | [LoRA Implementation](./LoRA) |

## Some useful resources.
List of resources, that I found helpful while understanding and coding the concepts.

1. Attention is all you need (Transformer) - Model explanation (including math), Inference and Training by Umar jamil: [Youtube](https://www.youtube.com/watch?v=bCz4OMemCcA&t=3016s).

2. Coding a Transformer from scratch on PyTorch, with full explanation, training and inference by Umar jamil: [Youtube](https://www.youtube.com/watch?v=ISNdQcPhsts).

3. Let's build GPT: from scratch, in code, spelled out by Andrej Karpathy. [Youtube](https://www.youtube.com/watch?v=kCc8FmEb1nY).