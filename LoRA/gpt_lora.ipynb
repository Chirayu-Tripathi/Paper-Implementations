{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTkJ9yu7dv9n"
      },
      "source": [
        "# GPT2 with LoRA implementation in PyTorch\n",
        "\n",
        "Let's start by importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zv0IXoGydv9y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1edz4pbbdv92"
      },
      "source": [
        "Make the model deterministic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NQSpnumAdv94"
      },
      "outputs": [],
      "source": [
        "# Make torch deterministic\n",
        "_ = torch.manual_seed(0)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnf-cACXk31C",
        "outputId": "b75298f1-9cb6-4a5f-d143-0815b705f304"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    fast_tokenizer=True)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    config=config,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xWVomOVl0BZ",
        "outputId": "26266be0-a613-4845-9618-4d80fec82fb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "vsj7GCvZvgIh",
        "outputId": "c67b84c2-e636-4393-f6f2-cdaaf08420be"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'net' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-24985952bfc0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Save the count of the total number of parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_parameters_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_parameters_original\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "L_Pg0ieWmGRw"
      },
      "outputs": [],
      "source": [
        "class LoRAParametrization(nn.Module):\n",
        "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
        "        super().__init__()\n",
        "        # Section 4.1 of the paper:\n",
        "        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
        "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
        "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
        "\n",
        "        # Section 4.1 of the paper:\n",
        "        #   We then scale ∆Wx by α/r , where α is a constant in r.\n",
        "        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately.\n",
        "        #   As a result, we simply set α to the first r we try and do not tune it.\n",
        "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
        "        self.scale = alpha / rank\n",
        "        self.enabled = True\n",
        "\n",
        "    def forward(self, original_weights):\n",
        "        if self.enabled:\n",
        "            # Return W + (B*A)*scale\n",
        "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
        "        else:\n",
        "            return original_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "659bSSm8q-_a"
      },
      "outputs": [],
      "source": [
        "target_names = []\n",
        "for name, module in model.named_modules():\n",
        "    if \"attn.c_attn\" in name:\n",
        "        target_names.append(name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA-RUU2IsDWC",
        "outputId": "cff0db62-032d-4dc0-b72c-0b4f0f51c6e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['transformer.h.0.attn.c_attn',\n",
              " 'transformer.h.1.attn.c_attn',\n",
              " 'transformer.h.2.attn.c_attn',\n",
              " 'transformer.h.3.attn.c_attn',\n",
              " 'transformer.h.4.attn.c_attn',\n",
              " 'transformer.h.5.attn.c_attn',\n",
              " 'transformer.h.6.attn.c_attn',\n",
              " 'transformer.h.7.attn.c_attn',\n",
              " 'transformer.h.8.attn.c_attn',\n",
              " 'transformer.h.9.attn.c_attn',\n",
              " 'transformer.h.10.attn.c_attn',\n",
              " 'transformer.h.11.attn.c_attn']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7MJVXELNtoDG"
      },
      "outputs": [],
      "source": [
        "target_modules = [model.transformer.h[0].attn.c_attn,\n",
        " model.transformer.h[1].attn.c_attn,\n",
        " model.transformer.h[2].attn.c_attn,\n",
        " model.transformer.h[3].attn.c_attn,\n",
        " model.transformer.h[4].attn.c_attn,\n",
        " model.transformer.h[5].attn.c_attn,\n",
        " model.transformer.h[6].attn.c_attn,\n",
        " model.transformer.h[7].attn.c_attn,\n",
        " model.transformer.h[8].attn.c_attn,\n",
        " model.transformer.h[9].attn.c_attn,\n",
        " model.transformer.h[10].attn.c_attn,\n",
        " model.transformer.h[11].attn.c_attn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRSyxsiirIUK",
        "outputId": "398959d8-638a-4aa4-b898-76b04753560b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D(),\n",
              " Conv1D()]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frNP1JO7vmDe",
        "outputId": "0e56050d-b19b-429d-d2c8-691a49e6836a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 2: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 3: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 4: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 5: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 6: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 7: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 8: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 9: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 10: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 11: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Layer 12: W: torch.Size([768, 2304]) + B: torch.Size([2304])\n",
            "Total number of parameters: 21,261,312\n"
          ]
        }
      ],
      "source": [
        "# Print the size of the weights matrices of the network\n",
        "# Save the count of the total number of parameters\n",
        "total_parameters_original = 0\n",
        "for index, layer in enumerate(target_modules):\n",
        "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
        "print(f'Total number of parameters: {total_parameters_original:,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "De7wOSsEq4Bw"
      },
      "outputs": [],
      "source": [
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
        "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
        "\n",
        "    # From section 4.2 of the paper:\n",
        "    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n",
        "    #   [...]\n",
        "    #   We leave the empirical investigation of [...], and biases to a future work.\n",
        "\n",
        "    features_in, features_out = layer.weight.shape\n",
        "    return LoRAParametrization(\n",
        "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
        "    )\n",
        "\n",
        "for target_module in target_modules:\n",
        "    parametrize.register_parametrization(\n",
        "        target_module, \"weight\", linear_layer_parameterization(target_module, device)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def enable_disable_lora(enabled=True):\n",
        "    for layer in target_modules:\n",
        "        layer.parametrizations[\"weight\"][0].enabled = enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XYOsRe5vtay",
        "outputId": "ecafe873-c049-4a85-c940-06ea0aa1627b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 2: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 3: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 4: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 5: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 6: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 7: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 8: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 9: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 10: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 11: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Layer 12: W: torch.Size([768, 2304]) + B: torch.Size([2304]) + Lora_A: torch.Size([1, 2304]) + Lora_B: torch.Size([768, 1])\n",
            "Total number of parameters (original): 21,261,312\n",
            "Total number of parameters (original + LoRA): 21,298,176\n",
            "Parameters introduced by LoRA: 36,864\n",
            "Parameters incremment: 0.173%\n"
          ]
        }
      ],
      "source": [
        "total_parameters_lora = 0\n",
        "total_parameters_non_lora = 0\n",
        "for index, layer in enumerate(target_modules):\n",
        "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
        "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(\n",
        "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
        "    )\n",
        "# The non-LoRA parameters count must match the original network\n",
        "assert total_parameters_non_lora == total_parameters_original\n",
        "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
        "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
        "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
        "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
        "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0t6jErdu_Rk",
        "outputId": "8eadbeb8-0d4b-4f13-aeaa-135aecaf6c49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.wte.weight\n",
            "transformer.wpe.weight\n",
            "transformer.h.0.ln_1.weight\n",
            "transformer.h.0.ln_1.bias\n",
            "transformer.h.0.attn.c_attn.bias\n",
            "transformer.h.0.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.0.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.0.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.0.attn.c_proj.weight\n",
            "transformer.h.0.attn.c_proj.bias\n",
            "transformer.h.0.ln_2.weight\n",
            "transformer.h.0.ln_2.bias\n",
            "transformer.h.0.mlp.c_fc.weight\n",
            "transformer.h.0.mlp.c_fc.bias\n",
            "transformer.h.0.mlp.c_proj.weight\n",
            "transformer.h.0.mlp.c_proj.bias\n",
            "transformer.h.1.ln_1.weight\n",
            "transformer.h.1.ln_1.bias\n",
            "transformer.h.1.attn.c_attn.bias\n",
            "transformer.h.1.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.1.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.1.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.1.attn.c_proj.weight\n",
            "transformer.h.1.attn.c_proj.bias\n",
            "transformer.h.1.ln_2.weight\n",
            "transformer.h.1.ln_2.bias\n",
            "transformer.h.1.mlp.c_fc.weight\n",
            "transformer.h.1.mlp.c_fc.bias\n",
            "transformer.h.1.mlp.c_proj.weight\n",
            "transformer.h.1.mlp.c_proj.bias\n",
            "transformer.h.2.ln_1.weight\n",
            "transformer.h.2.ln_1.bias\n",
            "transformer.h.2.attn.c_attn.bias\n",
            "transformer.h.2.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.2.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.2.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.2.attn.c_proj.weight\n",
            "transformer.h.2.attn.c_proj.bias\n",
            "transformer.h.2.ln_2.weight\n",
            "transformer.h.2.ln_2.bias\n",
            "transformer.h.2.mlp.c_fc.weight\n",
            "transformer.h.2.mlp.c_fc.bias\n",
            "transformer.h.2.mlp.c_proj.weight\n",
            "transformer.h.2.mlp.c_proj.bias\n",
            "transformer.h.3.ln_1.weight\n",
            "transformer.h.3.ln_1.bias\n",
            "transformer.h.3.attn.c_attn.bias\n",
            "transformer.h.3.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.3.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.3.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.3.attn.c_proj.weight\n",
            "transformer.h.3.attn.c_proj.bias\n",
            "transformer.h.3.ln_2.weight\n",
            "transformer.h.3.ln_2.bias\n",
            "transformer.h.3.mlp.c_fc.weight\n",
            "transformer.h.3.mlp.c_fc.bias\n",
            "transformer.h.3.mlp.c_proj.weight\n",
            "transformer.h.3.mlp.c_proj.bias\n",
            "transformer.h.4.ln_1.weight\n",
            "transformer.h.4.ln_1.bias\n",
            "transformer.h.4.attn.c_attn.bias\n",
            "transformer.h.4.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.4.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.4.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.4.attn.c_proj.weight\n",
            "transformer.h.4.attn.c_proj.bias\n",
            "transformer.h.4.ln_2.weight\n",
            "transformer.h.4.ln_2.bias\n",
            "transformer.h.4.mlp.c_fc.weight\n",
            "transformer.h.4.mlp.c_fc.bias\n",
            "transformer.h.4.mlp.c_proj.weight\n",
            "transformer.h.4.mlp.c_proj.bias\n",
            "transformer.h.5.ln_1.weight\n",
            "transformer.h.5.ln_1.bias\n",
            "transformer.h.5.attn.c_attn.bias\n",
            "transformer.h.5.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.5.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.5.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.5.attn.c_proj.weight\n",
            "transformer.h.5.attn.c_proj.bias\n",
            "transformer.h.5.ln_2.weight\n",
            "transformer.h.5.ln_2.bias\n",
            "transformer.h.5.mlp.c_fc.weight\n",
            "transformer.h.5.mlp.c_fc.bias\n",
            "transformer.h.5.mlp.c_proj.weight\n",
            "transformer.h.5.mlp.c_proj.bias\n",
            "transformer.h.6.ln_1.weight\n",
            "transformer.h.6.ln_1.bias\n",
            "transformer.h.6.attn.c_attn.bias\n",
            "transformer.h.6.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.6.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.6.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.6.attn.c_proj.weight\n",
            "transformer.h.6.attn.c_proj.bias\n",
            "transformer.h.6.ln_2.weight\n",
            "transformer.h.6.ln_2.bias\n",
            "transformer.h.6.mlp.c_fc.weight\n",
            "transformer.h.6.mlp.c_fc.bias\n",
            "transformer.h.6.mlp.c_proj.weight\n",
            "transformer.h.6.mlp.c_proj.bias\n",
            "transformer.h.7.ln_1.weight\n",
            "transformer.h.7.ln_1.bias\n",
            "transformer.h.7.attn.c_attn.bias\n",
            "transformer.h.7.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.7.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.7.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.7.attn.c_proj.weight\n",
            "transformer.h.7.attn.c_proj.bias\n",
            "transformer.h.7.ln_2.weight\n",
            "transformer.h.7.ln_2.bias\n",
            "transformer.h.7.mlp.c_fc.weight\n",
            "transformer.h.7.mlp.c_fc.bias\n",
            "transformer.h.7.mlp.c_proj.weight\n",
            "transformer.h.7.mlp.c_proj.bias\n",
            "transformer.h.8.ln_1.weight\n",
            "transformer.h.8.ln_1.bias\n",
            "transformer.h.8.attn.c_attn.bias\n",
            "transformer.h.8.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.8.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.8.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.8.attn.c_proj.weight\n",
            "transformer.h.8.attn.c_proj.bias\n",
            "transformer.h.8.ln_2.weight\n",
            "transformer.h.8.ln_2.bias\n",
            "transformer.h.8.mlp.c_fc.weight\n",
            "transformer.h.8.mlp.c_fc.bias\n",
            "transformer.h.8.mlp.c_proj.weight\n",
            "transformer.h.8.mlp.c_proj.bias\n",
            "transformer.h.9.ln_1.weight\n",
            "transformer.h.9.ln_1.bias\n",
            "transformer.h.9.attn.c_attn.bias\n",
            "transformer.h.9.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.9.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.9.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.9.attn.c_proj.weight\n",
            "transformer.h.9.attn.c_proj.bias\n",
            "transformer.h.9.ln_2.weight\n",
            "transformer.h.9.ln_2.bias\n",
            "transformer.h.9.mlp.c_fc.weight\n",
            "transformer.h.9.mlp.c_fc.bias\n",
            "transformer.h.9.mlp.c_proj.weight\n",
            "transformer.h.9.mlp.c_proj.bias\n",
            "transformer.h.10.ln_1.weight\n",
            "transformer.h.10.ln_1.bias\n",
            "transformer.h.10.attn.c_attn.bias\n",
            "transformer.h.10.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.10.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.10.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.10.attn.c_proj.weight\n",
            "transformer.h.10.attn.c_proj.bias\n",
            "transformer.h.10.ln_2.weight\n",
            "transformer.h.10.ln_2.bias\n",
            "transformer.h.10.mlp.c_fc.weight\n",
            "transformer.h.10.mlp.c_fc.bias\n",
            "transformer.h.10.mlp.c_proj.weight\n",
            "transformer.h.10.mlp.c_proj.bias\n",
            "transformer.h.11.ln_1.weight\n",
            "transformer.h.11.ln_1.bias\n",
            "transformer.h.11.attn.c_attn.bias\n",
            "transformer.h.11.attn.c_attn.parametrizations.weight.original\n",
            "transformer.h.11.attn.c_attn.parametrizations.weight.0.lora_A\n",
            "transformer.h.11.attn.c_attn.parametrizations.weight.0.lora_B\n",
            "transformer.h.11.attn.c_proj.weight\n",
            "transformer.h.11.attn.c_proj.bias\n",
            "transformer.h.11.ln_2.weight\n",
            "transformer.h.11.ln_2.bias\n",
            "transformer.h.11.mlp.c_fc.weight\n",
            "transformer.h.11.mlp.c_fc.bias\n",
            "transformer.h.11.mlp.c_proj.weight\n",
            "transformer.h.11.mlp.c_proj.bias\n",
            "transformer.ln_f.weight\n",
            "transformer.ln_f.bias\n"
          ]
        }
      ],
      "source": [
        "# Freeze the non-Lora parameters\n",
        "for name, param in model.named_parameters():\n",
        "  print(name)\n",
        "    # if 'lora' not in name:\n",
        "    #     print(f'Freezing non-LoRA parameter {name}')\n",
        "    #     param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M61ae98dv-E"
      },
      "source": [
        "Keep a copy of the original weights (cloning them) so later we can prove that a fine-tuning with LoRA doesn't alter the original weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2FTITOcdv-E"
      },
      "outputs": [],
      "source": [
        "original_weights = {}\n",
        "for name, param in net.named_parameters():\n",
        "    original_weights[name] = param.clone().detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXDkybQodv-G"
      },
      "source": [
        "Let's visualize how many parameters are in the original network, before introducing the LoRA matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcZZobPddv-H",
        "outputId": "2f1fb1fd-0602-4c0f-9b1d-374b805915df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
            "Total number of parameters: 2,807,010\n"
          ]
        }
      ],
      "source": [
        "# Print the size of the weights matrices of the network\n",
        "# Save the count of the total number of parameters\n",
        "total_parameters_original = 0\n",
        "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
        "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
        "print(f'Total number of parameters: {total_parameters_original:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWFO0Cl2dv-H"
      },
      "source": [
        "Define the LoRA parameterization as described in the paper.\n",
        "The full detail on how PyTorch parameterizations work is here: https://pytorch.org/tutorials/intermediate/parametrizations.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhTCHmN4dv-J"
      },
      "source": [
        "Add the parameterization to our network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yyhF3zGdv-J"
      },
      "source": [
        "Display the number of parameters added by LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVEEbn7Mdv-J",
        "outputId": "a19dbc88-eb0b-4d16-bfe1-8ba136235621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n",
            "Total number of parameters (original): 2,807,010\n",
            "Total number of parameters (original + LoRA): 2,813,804\n",
            "Parameters introduced by LoRA: 6,794\n",
            "Parameters incremment: 0.242%\n"
          ]
        }
      ],
      "source": [
        "total_parameters_lora = 0\n",
        "total_parameters_non_lora = 0\n",
        "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
        "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
        "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(\n",
        "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
        "    )\n",
        "# The non-LoRA parameters count must match the original network\n",
        "assert total_parameters_non_lora == total_parameters_original\n",
        "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
        "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
        "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
        "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
        "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0zb4CBdv-K"
      },
      "source": [
        "Freeze all the parameters of the original network and only fine tuning the ones introduced by LoRA. Then fine-tune the model on the digit 9 and only for 100 batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azq3DlJydv-K",
        "outputId": "b3ad9cb9-21ca-427e-9c0d-1a1339553efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing non-LoRA parameter linear1.bias\n",
            "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear2.bias\n",
            "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear3.bias\n",
            "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s, loss=0.188] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 238.78it/s, loss=0.089] \n"
          ]
        }
      ],
      "source": [
        "# Freeze the non-Lora parameters\n",
        "for name, param in net.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        print(f'Freezing non-LoRA parameter {name}')\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Load the MNIST dataset again, by keeping only the digit 9\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "exclude_indices = mnist_trainset.targets == 9\n",
        "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
        "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
        "# Create a dataloader for the training\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
        "train(train_loader, net, epochs=1, total_iterations_limit=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU2FOpundv-K"
      },
      "source": [
        "Verify that the fine-tuning didn't alter the original weights, but only the ones introduced by LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sun5ZClFdv-K"
      },
      "outputs": [],
      "source": [
        "# Check that the frozen parameters are still unchanged by the finetuning\n",
        "assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
        "assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
        "assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n",
        "\n",
        "enable_disable_lora(enabled=True)\n",
        "# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n",
        "# The original weights have been moved to net.linear1.parametrizations.weight.original\n",
        "# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
        "assert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n",
        "\n",
        "enable_disable_lora(enabled=False)\n",
        "# If we disable LoRA, the linear1.weight is the original one\n",
        "assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZj67j3Mdv-L"
      },
      "source": [
        "Test the network with LoRA enabled (the digit 9 should be classified better)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g95LIedJdv-M"
      },
      "source": [
        "Test the network with LoRA disabled (the accuracy and errors counts must be the same as the original network)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch-lora",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
